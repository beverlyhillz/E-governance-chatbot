{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test, vectorizer_path):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=[1,2],min_df=5,max_df=0.9,token_pattern='(\\S+)')\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    X_train=tfidf_vectorizer.transform(X_train)\n",
    "    X_test=tfidf_vectorizer.transform(X_test)\n",
    "    pickle.dump(tfidf_vectorizer, open(vectorizer_path, 'wb'))\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 310\n",
    "dialogue_df = pd.read_csv('data/dialogues.tsv', sep='\\t').sample(sample_size, random_state=0)\n",
    "questions_df = pd.read_csv('data/questions.csv', sep=',').sample(sample_size, random_state=0)\n",
    "dialogue_df['text'] = [text_prepare(x) for x in dialogue_df['text']]\n",
    "questions_df['title'] = [text_prepare(x) for x in questions_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 558, test size = 62\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([dialogue_df['text'].values, questions_df['title'].values])\n",
    "y = ['dialogue'] * dialogue_df.shape[0] + ['questions'] * questions_df.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, RESOURCE_PATH['TFIDF_VECTORIZER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8924731182795699"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_recognizer=LogisticRegression(penalty='l2',C=10, random_state=0).fit(X_train_tfidf, y_train)\n",
    "accuracy_score(y_train, intent_recognizer.predict(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.8870967741935484\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy = {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(intent_recognizer, open(RESOURCE_PATH['INTENT_RECOGNIZER'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi={'awas':'home','pradhan':'prime','mantri':'minister','yojana':'scheme','aam':'normal','admi':'man','bima':'insurance','varishtha':'old','vaya':'age','vandana':'prayer','jeevan':'life','jyoti':'light','fasal':'crop','suraksha':'security','jan':'people','dhan':'money','arogya':'health','mudra':'currency','rojgar':'employment','swasthya':'health','rastriya':'national'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['varishtha pension bima yojana', 'pradhan mantri mudra yojana', 'scheme liberation rehabilitation scavenger', 'pradhan mantri jeevan jyoti bima yojana', 'aam admi bima yojana', 'pradhan mantri suraksh bima yojna', 'national social assistance programme insurance', 'atal pension yojana', 'employment state insurance scheme', 'national pension system', 'pradhan mantri fasal bima yojana', 'pradhan mantri jan arogya', 'general insurance', 'genaral home loan', 'pradhan mantri jandhan yojana', 'vida lakshmi yojana', 'stand india buissness', 'prime ministers rozgar yojana', 'pradhan mantri awas yojana', 'pradhan mantri vaya vandana yojana', 'rashtriya swasthya bima yojana', 'pension insurance', 'small business loan', 'house loan', 'life insurance', 'accident disablity death insurance', 'health insurance', 'insurance', 'pension scheme', 'health insurance', 'senior citizen insurance', 'agriculture crop insurance', 'general insurance faq', 'genaral home loan faq', 'accidental death insurance', 'education loan', 'business loan backward', 'buissness loan', 'home loan', 'pension scheme insurance']\n"
     ]
    }
   ],
   "source": [
    "X = questions_df['scheme'].unique()\n",
    "Cat = questions_df['category'].unique()\n",
    "New=[]\n",
    "for c in X:\n",
    "    New.append(c)\n",
    "for c in Cat:\n",
    "    New.append(c)\n",
    "Q=[text_prepare(x) for x in New]\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in Q:\n",
    "    for j in i.split():\n",
    "        a.append(j)\n",
    "a=[i for i in set(a)]\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "b={}\n",
    "for i in range(len(a)):\n",
    "    b[a[i]]=[int(i==j) for j in range(len(a))]\n",
    "R=[]\n",
    "for sch in Q:\n",
    "    arr=sch.split()\n",
    "    vec = np.zeros(len(a))\n",
    "    i=0\n",
    "    for word in arr:\n",
    "        vec=vec+b[word]\n",
    "        i+=1\n",
    "            \n",
    "    if i!=0:\n",
    "        vec= vec/i\n",
    "    R.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi={'awas':'home','pradhan':'prime','mantri':'minister','yojana':'scheme','aam':'normal','admi':'man','bima':'insurance','varishtha':'old','vaya':'age','vandana':'prayer','jeevan':'life','jyoti':'light','fasal':'crop','suraksha':'security','jan':'people','dhan':'money','arogya':'health','mudra':'currency','rojgar':'employment','swasthya':'health','rastriya':'national'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "st=string.ascii_lowercase\n",
    "chemd={}\n",
    "for i in st:\n",
    "    chemd[i]=[int(j==i) for j in st]\n",
    "d=[]\n",
    "for j in a:\n",
    "    s=np.zeros(26)\n",
    "    for i in j:\n",
    "        s=s+chemd[i]\n",
    "    d.append(s/len(j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_char(question, embeddings, dim):\n",
    "    \"\"\"Transforms a string to an embedding by averaging word embeddings.\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    arr=question.split()\n",
    "    vec = np.zeros(dim)\n",
    "    i=0\n",
    "    for word in arr:\n",
    "        if word in embeddings:\n",
    "            vec+=embeddings[word]\n",
    "            i+=1\n",
    "        else:\n",
    "            best_scheme = np.argmax(cosine_similarity(s, d)[0])\n",
    "            \n",
    "            if a[best_scheme] in embeddings:\n",
    "                vec=vec+embeddings[a[best_scheme]]\n",
    "                i+=1\n",
    "    if i!=0:\n",
    "        return vec/i\n",
    "    else:\n",
    "        return vec\n",
    "s=wrd2vec('primr')\n",
    "best_scheme = np.argmax(cosine_similarity(s, d)[0])\n",
    "a[best_scheme]\n",
    "R=[]\n",
    "for sch in Q:\n",
    "    arr=sch.split()\n",
    "    vec = np.zeros(300)\n",
    "    i=0\n",
    "    for word in arr:\n",
    "        if word in word_embeddings:\n",
    "            vec=vec+word_embeddings[word]\n",
    "            i+=1\n",
    "            \n",
    "    if i!=0:\n",
    "        vec= vec/i\n",
    "    R.append(vec)\n",
    "def predict (inp):\n",
    "    W=question_to_vec_char(inp, word_embeddings, 300).reshape(1,-1)\n",
    "    best_scheme = np.argmax(cosine_similarity(W, R)[0])\n",
    "    return (Q[best_scheme])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#finalembed_df=pd.read_csv(\"dictionary.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvData=[]\n",
    "for i in finalembed_df['word']:\n",
    "    if i.lower() in finalembed_df:\n",
    "        w=finalembed_df[i.lower()]\n",
    "        csvRow=[i.lower()]\n",
    "        for j in w:\n",
    "            csvRow.append(j)\n",
    "        csvData.append(csvRow)\n",
    "    \n",
    "with open('finalembed.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings,embeddings_dim=load_embeddings('word_embeddings.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('data/questions.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_category = posts_df.groupby('category').count()['title']\n",
    "counts_by_scheme = posts_df.groupby('scheme').count()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(RESOURCE_PATH['SCHEME_EMBEDDINGS_FOLDER'], exist_ok=True)\n",
    "\n",
    "for scheme, count in counts_by_scheme.items():\n",
    "    scheme_posts = posts_df[posts_df['scheme'] == scheme]\n",
    "    \n",
    "    scheme_post_ids = scheme_posts['post_id'].tolist()\n",
    "    \n",
    "    scheme_vectors = np.zeros((count, embeddings_dim), dtype=np.float32)\n",
    "    for i, title in enumerate(scheme_posts['title']):\n",
    "        scheme_vectors[i, :] = question_to_vec(title, starspace_embeddings, embeddings_dim)\n",
    "\n",
    "    # Dump post ids and vectors to a file.\n",
    "    filename = os.path.join(RESOURCE_PATH['SCHEME_EMBEDDINGS_FOLDER'], os.path.normpath('%s.pkl' % scheme.lower()))\n",
    "    pickle.dump((scheme_post_ids, scheme_vectors), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(RESOURCE_PATH['CATEGORY_EMBEDDINGS_FOLDER'], exist_ok=True)\n",
    "\n",
    "for category, count in counts_by_category.items():\n",
    "    category_posts = posts_df[posts_df['category'] == category]\n",
    "    \n",
    "    category_post_ids = category_posts['post_id'].tolist()\n",
    "    \n",
    "    category_vectors = np.zeros((count, embeddings_dim), dtype=np.float32)\n",
    "    for i, title in enumerate(category_posts['title']):\n",
    "        category_vectors[i, :] = question_to_vec(title, starspace_embeddings, embeddings_dim)\n",
    "\n",
    "    # Dump post ids and vectors to a file.\n",
    "    filename = os.path.join(RESOURCE_PATH['CATEGORY_EMBEDDINGS_FOLDER'], os.path.normpath('%s.pkl' % category.lower()))\n",
    "    pickle.dump((category_post_ids, category_vectors), open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
